{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk7I5mwikem-",
        "outputId": "41cece19-bd2c-471c-8de4-1acb0e5f3564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed autoawq-0.2.9 datasets-3.6.0 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install -U autoawq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3OTtKdhxkhw_"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a JSON-only response system. Follow these rules absolutely:\n",
        "1. ONLY output valid, parseable JSON\n",
        "2. NEVER include text before or after the JSON\n",
        "3. NEVER include markdown code blocks or formatting\n",
        "4. NEVER include explanations\n",
        "5. NEVER extract dates\n",
        "6. If you can't fulfill a request, return {\"error\": \"error message\"}\n",
        "7. Output should always be a single JSON object\n",
        "\n",
        "For address requests, use this format:\n",
        "{\n",
        "    \"address\": {\n",
        "        \"license\": \"B1231241\",\n",
        "        \"Address\": \"X City\",\n",
        "        \"Sex\": \"Male\",\n",
        "        \"Weight\": \"X\",\n",
        "        \"Height\": \"X\"\n",
        "    }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VV-pAtIRklVg"
      },
      "outputs": [],
      "source": [
        "test_instruction = '''extract NER:\n",
        "        California\n",
        "        DRIVER LICENSe\n",
        "        dl 11234568\n",
        "        CLASS C\n",
        "        EXP 08/31/2014\n",
        "        END NONE\n",
        "        LNCARDHOLDER FNIMA\n",
        "        2570 24TH STREET ANYTOWN, CA 95818\n",
        "        doB 08/31/1977 RSTR NONE\n",
        "        08311977\n",
        "        VETERAN\n",
        "        Cordhslde\n",
        "        SEX F HGT 5'-05\"\n",
        "        HAIR BRN WGT 125 lb\n",
        "        EYES BRN\n",
        "        DD 00/00/0000NNNAN/ANFD/YY\n",
        "        ISS 08/31/2009\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AWQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3UMOnh4kmGI",
        "outputId": "09ecfe6d-99f7-4805-8d43-d818e975bb74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch.bfloat16` is not supported for AWQ kernels yet. Casting to `torch.float16`.\n",
            "/usr/local/lib/python3.11/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
            "I have left this message as the final dev message to help you transition.\n",
            "\n",
            "Important Notice:\n",
            "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
            "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
            "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
            "\n",
            "Alternative:\n",
            "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
            "\n",
            "For further inquiries, feel free to reach out:\n",
            "- X: https://x.com/casper_hansen_\n",
            "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
            "\n",
            "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: ```json\n",
            "{\n",
            "    \"NER\": {\n",
            "        \"Location\": {\n",
            "            \"State\": \"California\",\n",
            "            \"Street\": \"24TH STREET\",\n",
            "            \"City\": \"ANYTOWN\",\n",
            "            \"Zip\": \"95818\"\n",
            "        },\n",
            "        \"Identification\": {\n",
            "            \"License\": \"DL 11234568\",\n",
            "            \"Class\": \"C\",\n",
            "            \"Expiration\": \"08/31/2014\",\n",
            "            \"LicenseHolder\": {\n",
            "                \"Name\": \"FNIMA\",\n",
            "\n",
            "Execution time: 11.88 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-AWQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define your prompts\n",
        "# system_prompt = \"You are a helpful AI assistant.\"\n",
        "# test_instruction = \"Explain what machine learning is in simple terms.\"\n",
        "\n",
        "# Prepare messages\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": test_instruction}\n",
        "]\n",
        "\n",
        "# Tokenize input\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Generate response\n",
        "start_time = time.time()\n",
        "with torch.no_grad():  # Save memory during inference\n",
        "    output = model.generate(\n",
        "        input_ids.to(device),\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id  # Handle padding\n",
        "    )\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Decode and print only the new tokens\n",
        "response = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "print(\"Response:\", response)\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3WoC2kNkqaZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
